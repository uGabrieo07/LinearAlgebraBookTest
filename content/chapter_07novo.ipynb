{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "id": "9a0a7d5e-60af-4ebc-9bb8-b55227523a1e",
      "cell_type": "markdown",
      "source": "# The Singular Value Decomposition\n\nMatrix decompositions are helpful in many ways. The\n$\\mathbf{P}\\mathbf{L}\\mathbf{U}$ and the spectral decompositions in\nchapters <a href=\"#chap:Ax=b\" data-reference-type=\"ref\"\ndata-reference=\"chap:Ax=b\">[chap:Ax=b]</a>\nand <a href=\"#chap:eigenvalues_eigenvectors\" data-reference-type=\"ref\"\ndata-reference=\"chap:eigenvalues_eigenvectors\">[chap:eigenvalues_eigenvectors]</a>\nare just two examples. In this chapter, we will present a broadly\napplicable and very general factorization of matrices as the product of\nan orthogonal matrix $\\mathbf{U}$, a diagonal matrix\n$\\boldsymbol{\\Sigma}$, and another orthogonal matrix\n$\\mathbf{V}^{\\rm T}$. This is called the singular value decomposition\n(SVD), and its roots date back to the nineteenth century, even before\nthe widespread use of matrices. Although the definition of the SVD\ndecomposition is straightforward, the proof of its existence is\nenlightening.\n\n## Definition and Existence of the SVD\n\nLet $\\mathbf{A}:\\mathbb{R}^n\\rightarrow\\mathbb{R}^m$ be a linear\ntransformation induced by matrix $\\mathbf{A}$. We maintain that\n$\\mathbf{A}$ can be factored into\n$$\\mathbf{A}= \\mathbf{U}\\boldsymbol{\\Sigma}\\mathbf{V}^{\\rm T}\n  \\label{eq:svd}$$ where $\\mathbf{U}$ and $\\mathbf{V}$ are orthogonal\nmatrices and $\\boldsymbol{\\Sigma}$ is an $m\\times n$ matrix with all\nentries equal to zero, except those in its diagonal, $\\sigma_{ii}$,\nwhich may be different from zero, but never negative. These are called\nthe singular values of the linear transformation $\\mathbf{A}$. We will\nshow below that $\\mathbf{U}$ and $\\mathbf{V}$ are orthogonal matrices\nwhose columns are the normalized eigenvectors of\n$\\mathbf{A}\\mathbf{A}^{\\rm T}$ and $\\mathbf{A}^{\\rm T}\\mathbf{A}$,\nrespectively. $\\boldsymbol{\\Sigma}$ is an $m\\times n$ matrix with the\npositive square roots of their eigenvalues on the diagonal, and zeros\nelsewhere.\n\nIn order to go into the details and prove the existence of the\ndecomposition, we will follow a strategy closely related to that offered\nby Beltrami [1]  \\[@Stew1993\\].\n\nLet $\\mathbf{x}\\in\\mathbb{R}^m$ and $\\mathbf{y}\\in\\mathbb{R}^n$ be\narbitrary vectors. We can write a bilinear function\n$f(\\mathbf{x},\\mathbf{y}):\\mathbb{R}^m\\times\\mathbb{R}^n\\rightarrow\\mathbb{R}$\nas $$f(\\mathbf{x},\\mathbf{y})=\\mathbf{x}^{\\rm T}\\mathbf{A}\\mathbf{y}$$\n\nNow let $\\{\\mathbf{u}_i\\},\\ i=1,\\ 2,\\ \\ldots,\\ m$ and\n$\\{\\mathbf{v}_i\\},\\ i=1,\\ 2,\\ \\ldots,\\ n$ be orthonormal bases of\n$\\mathbb{R}^m$ and $\\mathbb{R}^n$, respectively. Vectors $\\mathbf{x}$\nand $\\mathbf{y}$ can be written as linear combinations of the vectors in\nthe corresponding basis as $$\\mathbf{x}=\\mathbf{U}\\boldsymbol{\\xi}$$ and\n$$\\mathbf{y}=\\mathbf{V}\\boldsymbol{\\eta}$$ where matrices $\\mathbf{U}$\nand $\\mathbf{V}$ contain the basis’ vectors in their columns, as\nfollows:\n$$\\mathbf{U}=\\left[\\mathbf{u}_1\\ \\ \\mathbf{u}_2\\ \\ \\cdots\\ \\ \\mathbf{u}_m\\right]$$\nand\n$$\\mathbf{V}=\\left[\\mathbf{v}_1\\ \\ \\mathbf{v}_2\\ \\ \\cdots\\ \\ \\mathbf{v}_n\\right]$$\nTherefore, the bilinear function becomes $$\\begin{split}\n  f(\\mathbf{x},\\mathbf{y})&=\\boldsymbol{\\xi}^{\\rm T}\\mathbf{U}^{\\rm T}\\mathbf{A}\\mathbf{V}\\boldsymbol{\\eta}\\\\\n              &=\\boldsymbol{\\xi}^{\\rm T}\\boldsymbol{\\Sigma}\\boldsymbol{\\eta}\n\\end{split}$$ where\n$$\\boldsymbol{\\Sigma}=\\mathbf{U}^{\\rm T}\\mathbf{A}\\mathbf{V}$$\n\nGiven that $\\mathbf{U}$ and $\\mathbf{V}$ are orthogonal matrices, then\n$$\\mathbf{U}^{\\rm T}\\mathbf{A}=\n  \\boldsymbol{\\Sigma}\\mathbf{V}^{\\rm T}\\Rightarrow \\mathbf{U}^{\\rm T}\\mathbf{A}\\mathbf{A}^{\\rm T}\\mathbf{U}=\\boldsymbol{\\Sigma}\\boldsymbol{\\Sigma}^{\\rm T}$$\nand $$\\mathbf{A}\\mathbf{V}=\n  \\mathbf{U}\\boldsymbol{\\Sigma}\\Rightarrow \\mathbf{V}^{\\rm T}\\mathbf{A}^{\\rm T}\\mathbf{A}\\mathbf{V}=\\boldsymbol{\\Sigma}^{\\rm T}\\boldsymbol{\\Sigma}$$\nFurthermore, as $\\mathbf{A}\\mathbf{A}^{\\rm T}$ and\n$\\mathbf{A}^{\\rm T}\\mathbf{A}$ are both symmetric, we can choose\n$\\{\\mathbf{u}_i\\}$ and $\\{\\mathbf{v}_i\\}$ as their respective\northonormal eigenvectors. As a consequence of this choice, by the\nspectral decomposition seen in\nsection <a href=\"#sec:sym_matrices\" data-reference-type=\"ref\"\ndata-reference=\"sec:sym_matrices\">[sec:sym_matrices]</a>, matrices\n$\\boldsymbol{\\Sigma}\\boldsymbol{\\Sigma}^{\\rm T}=\\boldsymbol{\\Lambda}$\nand\n$\\boldsymbol{\\Sigma}^{\\rm T}\\boldsymbol{\\Sigma}=\\boldsymbol{\\Lambda}^\\prime$\nare both diagonal and carry the eigenvalues of\n$\\mathbf{A}\\mathbf{A}^{\\rm T}$ and $\\mathbf{A}^{\\rm T}\\mathbf{A}$,\nrespectively. Therefore\n$$\\mathbf{A}\\mathbf{A}^{\\rm T}\\mathbf{u}_i=\\lambda_i\\mathbf{u}_i \n  \\Rightarrow \n  \\mathbf{A}^{\\rm T}\\mathbf{A}\\mathbf{A}^{\\rm T}\\mathbf{u}_i=\\lambda_i\\mathbf{A}^{\\rm T}\\mathbf{u}_i$$\nFrom the equation above, we realize that $\\mathbf{A}\\mathbf{A}^{\\rm T}$\nand $\\mathbf{A}^{\\rm T}\\mathbf{A}$ share the same eigenvalues\n$\\{\\lambda_i\\}$, $i=1,\\ \\ldots,\\ r\\le\\min(m,n)$, associated with\neigenvectors which are related by the linear transformation\n$\\mathbf{A}^{\\rm T}$. Let us denote $\\bar{\\mathbf{v}}_i$, an\nunnormalized version of the eigenvector $\\mathbf{v}_i$ of\n$\\mathbf{A}^{\\rm T}\\mathbf{A}$, as $$\\label{eq:vi_bar}\n  \\bar{\\mathbf{v}}_i=\\mathbf{A}^{\\rm T}\\mathbf{u}_i$$\n\nOn the other hand,\n$$\\boldsymbol{\\Sigma}=\\mathbf{U}^{\\rm T}\\mathbf{A}\\mathbf{V}\\Rightarrow \\mathbf{A}^{\\rm T}\\mathbf{U}= \\mathbf{V}\\boldsymbol{\\Sigma}^{\\rm T}$$\nor, equivalently, $$\\label{eq:vi}\n  \\mathbf{A}^{\\rm T}\\left[\\mathbf{u}_1\\ \\ \\mathbf{u}_2 \\ \\ \\cdots \\ \\ \\mathbf{u}_m\\right] =\n  \\left[\\mathbf{v}_1\\ \\ \\mathbf{v}_2\\ \\ \\cdots \\ \\ \\mathbf{v}_n\\right]\\boldsymbol{\\Sigma}^{\\rm T}\n  \\Rightarrow \n  \\mathbf{A}^{\\rm T}\\mathbf{u}_i = \\sum_{j=1}^{n}\\sigma_{ji}\\mathbf{v}_i\n  \\quad \n  i=1,\\ 2,\\ \\ldots,\\ m$$ From\nequations <a href=\"#eq:vi_bar\" data-reference-type=\"ref\"\ndata-reference=\"eq:vi_bar\">[eq:vi_bar]</a>\nand <a href=\"#eq:vi\" data-reference-type=\"ref\"\ndata-reference=\"eq:vi\">[eq:vi]</a>, we realize that $\\sigma_{ji}=0$ for\nall $i\\ne j$. Therefore $\\boldsymbol{\\Sigma}$ is “nonsquare diagonal”\nand can be constructed with the positive square roots of the nonzero\neigenvalues of either $\\mathbf{A}\\mathbf{A}^{\\rm T}$, or\n$\\mathbf{A}^{\\rm T}\\mathbf{A}$.\n\nAt this point, a note on the indexes feels necessary. In case $n<m$, the\nlast $m-n$ eigenvalues of $\\mathbf{A}\\mathbf{A}^{\\rm T}$ are equal to\n$\\lambda_{n+1}=\\lambda_{n+2}=\\cdots=\\lambda_m=0$. Similarly, for $m<n$,\nthe last $n-m$ eigenvalues of $\\mathbf{A}^{\\rm T}\\mathbf{A}$ are equal\nto $\\lambda_{m+1}=\\lambda_{m+2}=\\cdots=\\lambda_n=0$.\n\n## Calculating the SVD\n\nThe results presented in\nsection <a href=\"#sec:sym_matrices\" data-reference-type=\"ref\"\ndata-reference=\"sec:sym_matrices\">[sec:sym_matrices]</a> ensure that\nthere exist orthonormal eigenvectors of $\\mathbf{A}\\mathbf{A}^{\\rm T}$\nand $\\mathbf{A}^{\\rm T}\\mathbf{A}$ that form bases of $\\mathbb{R}^m$ and\n$\\mathbb{R}^n$, respectively, associated with real eigenvalues. The\nquadratic form of these symmetric matrices also guarantee that the\neigenvalues are nonnegative:\n$\\boldsymbol{\\Lambda}=\\boldsymbol{\\Sigma}\\boldsymbol{\\Sigma}^{\\rm T}$\nand\n$\\boldsymbol{\\Lambda}^\\prime=\\boldsymbol{\\Sigma}^{\\rm T}\\boldsymbol{\\Sigma}$.\n\nA generic algorithm for obtaining matrices $\\mathbf{U}$,\n$\\boldsymbol{\\Sigma}$, and $\\mathbf{V}$ is as follows.\n\n$\\{\\mathbf{u}_i\\}\\gets$ orthonormal eigenvectors of\n$\\mathbf{A}\\mathbf{A}^{\\rm T},\\ \\ i=1,\\ 2,\\ \\ldots,\\ m$, associated with\neigenvalues in nonincreasing order of magnitude, i.e.,\n$\\lambda_1\\ge\\lambda_2\\ge\\cdots\\ge\\lambda_m$\n$\\{\\sigma_{ii}\\}\\gets\\sqrt{\\lambda_i},\\ \\ i=1,\\ 2,\\ \\ldots,\\ \\min{(m,n)}$\n$\\{\\bar{\\mathbf{v}}_i\\}\\gets\\mathbf{A}^{\\rm T}\\mathbf{u}_i,\\ \\ i=1,\\ 2,\\ \\ldots,\\ \\min{(m,n)}$\n$\\{\\bar{\\mathbf{v}}_{m+1},\\ \\bar{\\mathbf{v}}_{m+2},\\ \\ldots,\\ \\bar{\\mathbf{v}}_n\\}\\gets$\northonormal vectors which are orthogonal to\n$\\{\\bar{\\mathbf{v}}_i\\},\\ \\ i=1,\\ 2,\\ \\ldots,\\ m$\n$\\{\\mathbf{v}_i\\}\\gets\\bar{\\mathbf{v}}_i/\\|\\bar{\\mathbf{v}}_i\\|,\\ \\ i=1,\\ 2,\\ \\ldots,\\ n$\n\n\n## Geometric considerations\n\n[1] E. Beltrami, *Sulle Funzioni Bilineari*, 1873 \\[@Belt1873\\].",
      "metadata": {}
    },
    {
      "id": "eeef0554",
      "cell_type": "markdown",
      "source": "## Applications\n\nOne of the main utilities of the SVD decomposition is to organize and comprehend the relevance of certain data. One of the fields in which this can be applied is image processing. If we observe a gray image as a matrix, we can apply SVD decomposition to the image, and observe in the ${\\Sigma}$ matrix the relevance of the pixels in each column to the composition of the image. \nThis procedure can help us properly reduce images, since it is made certain that the least relevant information is eliminated.\nSee this for yourself in the example below!",
      "metadata": {}
    },
    {
      "id": "68b33045",
      "cell_type": "code",
      "source": "from skimage import data\nfrom skimage.color import rgb2gray",
      "metadata": {},
      "outputs": [],
      "execution_count": 1
    },
    {
      "id": "a2dcd790",
      "cell_type": "code",
      "source": "from skimage import img_as_ubyte, img_as_float\ngray = {\"cat\":rgb2gray(img_as_float(data.chelsea())),\n       \"astronaut\": rgb2gray(img_as_float(data.astronaut())),\n       \"photographer\": data.camera(),\n       \"coffee\": rgb2gray(img_as_float(data.coffee()))}",
      "metadata": {},
      "outputs": [],
      "execution_count": 2
    },
    {
      "id": "9722ccb2",
      "cell_type": "code",
      "source": "import numpy as np\nfrom numpy.linalg import svd",
      "metadata": {},
      "outputs": [],
      "execution_count": 3
    },
    {
      "id": "41c806b0",
      "cell_type": "code",
      "source": "def decsvd(image, k):\n    U, S, V = svd(image, full_matrices=False)\n    product = np.dot(U[:,:k], np.dot(np.diag(S[:k]), V[:k,:]))\n    return product, S",
      "metadata": {},
      "outputs": [],
      "execution_count": 4
    },
    {
      "id": "f5d8e090",
      "cell_type": "code",
      "source": "import matplotlib.pyplot as plt",
      "metadata": {},
      "outputs": [],
      "execution_count": 5
    },
    {
      "id": "0c679354",
      "cell_type": "code",
      "source": "def show_img(img_name, k):\n    image = gray[img_name]\n    original_shape = image.shape\n    reconst_img, s = decsvd(image,k)\n    fig, axes = plt.subplots(1,2,figsize=(8,5))\n    axes[0].plot(s)\n    compression_ratio = 100.0*(k*(original_shape[0] + original_shape[1])+k)/(original_shape[0]*original_shape[1])\n    axes[1].set_title(\"compression ratio={:.2f}\".format(compression_ratio)+\"%\")\n    axes[1].imshow(reconst_img, cmap='gray')\n    axes[1].axis('off')\n    fig.tight_layout()\n    plt.show()",
      "metadata": {},
      "outputs": [],
      "execution_count": 6
    },
    {
      "id": "ec2cb3ab",
      "cell_type": "code",
      "source": "%pip install -q ipywidgets==8.0.7 ",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Note: you may need to restart the kernel to use updated packages.\n"
        }
      ],
      "execution_count": 7
    },
    {
      "id": "0e650727",
      "cell_type": "code",
      "source": "import ipywidgets as widgets\nfrom ipywidgets import interact",
      "metadata": {},
      "outputs": [],
      "execution_count": 8
    },
    {
      "id": "234c8953",
      "cell_type": "code",
      "source": "interact(show_img, img_name=list(gray.keys()), k=(1,300))",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1020576b00344183b37585758ba1f61d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "interactive(children=(Dropdown(description='img_name', options=('cat', 'astronaut', 'photographer', 'coffee'),…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "<function __main__.show_img(img_name, k)>"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "execution_count": 9
    },
    {
      "id": "3e58da9a",
      "cell_type": "markdown",
      "source": "If you change the value of k with the slider, you will see that the quality of the image changes. The graph next to the image illustrates the distribuition of the singular values. ",
      "metadata": {}
    }
  ]
}